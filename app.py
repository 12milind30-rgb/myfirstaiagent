import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
from mlxtend.frequent_patterns import apriori, association_rules
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from datetime import timedelta
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import warnings

warnings.filterwarnings("ignore")

# --------------------------------------------------
# PAGE CONFIG
# --------------------------------------------------
st.set_page_config(page_title="Mithas Intelligence 7.0", layout="wide")

# --------------------------------------------------
# DATA LOADING (FIXED: DATE + HOUR SAFETY)
# --------------------------------------------------
@st.cache_data
def load_data(file):
    df = pd.read_excel(file)

    col_map = {
        'Invoice No.': 'OrderID',
        'Item Name': 'ItemName',
        'Qty.': 'Quantity',
        'Final Total': 'TotalAmount',
        'Price': 'UnitPrice',
        'Category': 'Category',
        'Timestamp': 'Time',
        'Date': 'Date'
    }
    df = df.rename(columns=col_map)

    for c in ['Quantity', 'TotalAmount', 'UnitPrice']:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)

    # Date safety
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        df = df[df['Date'].notna()]
        df['DayOfWeek'] = df['Date'].dt.day_name()

    # Robust Hour extraction
    if 'Time' in df.columns:
        df['Hour'] = pd.to_datetime(df['Time'], errors='coerce').dt.hour
        df['Hour'] = df['Hour'].fillna(-1).astype(int)
        df = df[df['Hour'] >= 0]

    return df

# --------------------------------------------------
# PEAK HOUR FOR COMBOS (FIXED: QUANTITY-WEIGHTED)
# --------------------------------------------------
def get_peak_hour_for_pair(df, item_a, item_b):
    orders_a = set(df[df['ItemName'] == item_a]['OrderID'])
    orders_b = set(df[df['ItemName'] == item_b]['OrderID'])
    common_orders = orders_a.intersection(orders_b)

    if not common_orders:
        return "N/A"

    subset = df[df['OrderID'].isin(common_orders)]
    if 'Hour' not in subset.columns:
        return "N/A"

    peak = subset.groupby('Hour')['Quantity'].sum()
    if peak.empty:
        return "N/A"

    h = int(peak.idxmax())
    return f"{h:02d}:00 - {h+1:02d}:00"

# --------------------------------------------------
# COMBO ANALYSIS (FIXED: MEMORY GUARD + CACHED)
# --------------------------------------------------
@st.cache_data
def get_combo_analysis_full(df):
    # Prevent memory crash on large menus
    if df['ItemName'].nunique() > 200:
        return pd.DataFrame()

    top_items = (
        df.groupby('ItemName')['Quantity']
        .sum()
        .nlargest(150)
        .index
    )
    df = df[df['ItemName'].isin(top_items)]

    basket = (
        df.groupby(['OrderID', 'ItemName'])['Quantity']
        .sum()
        .unstack()
        .fillna(0)
        .astype(int)
    )

    basket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)
    frequent = apriori(basket_sets, min_support=0.005, use_colnames=True)
    if frequent.empty:
        return pd.DataFrame()

    rules = association_rules(frequent, metric="lift", min_threshold=1.05)

    rules['Item A'] = rules['antecedents'].apply(lambda x: list(x)[0])
    rules['Item B'] = rules['consequents'].apply(lambda x: list(x)[0])

    rules['pair'] = rules.apply(
        lambda x: tuple(sorted([x['Item A'], x['Item B']])), axis=1
    )
    rules = rules.drop_duplicates('pair')

    item_cat_map = df.set_index('ItemName')['Category'].to_dict()
    rules['Category A'] = rules['Item A'].map(item_cat_map)
    rules['Category B'] = rules['Item B'].map(item_cat_map)

    rules['Specific Item Combo'] = rules['Item A'] + " + " + rules['Item B']

    total_orders = df['OrderID'].nunique()
    rules['Times Sold Together'] = (rules['support'] * total_orders).astype(int)
    rules['Peak Hour'] = rules.apply(
        lambda x: get_peak_hour_for_pair(df, x['Item A'], x['Item B']), axis=1
    )

    return rules

def get_part3_strategy(rules_df):
    if rules_df.empty:
        return pd.DataFrame(), pd.DataFrame()

    proven = rules_df.sort_values(
        'Times Sold Together', ascending=False
    ).head(10).copy()
    proven['Strategy'] = "Bundle & Upsell (Proven Demand)"

    potential = rules_df[~rules_df.index.isin(proven.index)]
    potential = potential[potential['lift'] > 1.5] \
        .sort_values('lift', ascending=False) \
        .head(10).copy()
    potential['Strategy'] = "Market Aggressively (High Compatibility)"

    return proven, potential

# --------------------------------------------------
# OVERVIEW METRICS (FIXED: CORRECT AOV)
# --------------------------------------------------
def get_overview_metrics(df):
    order_totals = df.groupby('OrderID')['TotalAmount'].sum()

    total_rev = order_totals.sum()
    total_orders = order_totals.count()
    num_days = df['Date'].nunique()

    avg_rev_day = total_rev / num_days if num_days else 0
    avg_rev_week = total_rev / max(1, num_days / 7)
    aov = order_totals.mean() if total_orders else 0

    return total_rev, total_orders, avg_rev_day, avg_rev_week, aov

# --------------------------------------------------
# OTHER ANALYTICS (UNCHANGED)
# --------------------------------------------------
def get_star_items_with_hours(df):
    total_rev = df['TotalAmount'].sum()
    item_stats = df.groupby('ItemName').agg({'TotalAmount': 'sum'}).reset_index()
    item_stats['Contribution %'] = (item_stats['TotalAmount'] / total_rev) * 100
    item_stats = item_stats.sort_values('TotalAmount', ascending=False).head(20)

    peak_hours, peak_qtys = [], []
    for item in item_stats['ItemName']:
        item_data = df[df['ItemName'] == item]
        if 'Hour' in df.columns and not item_data.empty:
            hour_grouped = item_data.groupby('Hour')['Quantity'].sum()
            peak_hours.append(
                f"{int(hour_grouped.idxmax()):02d}:00 - {int(hour_grouped.idxmax())+1:02d}:00"
            )
            peak_qtys.append(hour_grouped.max())
        else:
            peak_hours.append("N/A")
            peak_qtys.append(0)

    item_stats['Peak Selling Hour'] = peak_hours
    item_stats['Qty Sold (Peak)'] = peak_qtys
    return item_stats

def analyze_peak_hour_items(df):
    if 'Hour' not in df.columns:
        return pd.DataFrame(), []

    hourly_rev = df.groupby('Hour')['TotalAmount'].sum()
    top_3_hours = hourly_rev.nlargest(3).index.tolist()

    peak_df = df[df['Hour'].isin(top_3_hours)]
    top_items = (
        peak_df.groupby('ItemName')['Quantity']
        .sum()
        .sort_values(ascending=False)
        .head(10)
        .reset_index()
    )
    top_items.columns = ['Item Name', 'Qty Sold in Peak Hours']

    return top_items, top_3_hours

def get_contribution_lists(df):
    total_rev = df['TotalAmount'].sum()

    cat_df = df.groupby('Category')['TotalAmount'].sum().reset_index()
    cat_df['Contribution'] = (cat_df['TotalAmount'] / total_rev) * 100
    cat_df = cat_df.sort_values('TotalAmount', ascending=False)

    item_df = df.groupby(['Category', 'ItemName'])['TotalAmount'].sum().reset_index()
    item_df['Contribution'] = (item_df['TotalAmount'] / total_rev) * 100
    item_df = item_df.sort_values(['Category', 'TotalAmount'], ascending=[True, False])

    return cat_df, item_df

@st.cache_data
def analyze_pareto_hierarchical(df):
    item_rev = df.groupby(['Category', 'ItemName'])['TotalAmount'].sum().reset_index()
    total_revenue = item_rev['TotalAmount'].sum()

    item_rev = item_rev.sort_values('TotalAmount', ascending=False)
    item_rev['Cumulative'] = item_rev['TotalAmount'].cumsum()
    item_rev['CumPerc'] = 100 * item_rev['Cumulative'] / total_revenue

    pareto_items = item_rev[item_rev['CumPerc'] <= 80].copy()

    total_unique_items = df['ItemName'].nunique()
    pareto_unique_items = pareto_items['ItemName'].nunique()

    ratio_text = (
        f"**{pareto_unique_items} items** (out of {total_unique_items}) "
        f"contribute to 80% of your revenue."
    )

    percentage_of_menu = (pareto_unique_items / total_unique_items) * 100

    cat_rev = df.groupby('Category')['TotalAmount'].sum().reset_index()
    cat_rev['CatContrib'] = (cat_rev['TotalAmount'] / total_revenue) * 100

    merged = pd.merge(
        pareto_items,
        cat_rev[['Category', 'CatContrib']],
        on='Category',
        how='left'
    )

    merged['ItemContrib'] = (merged['TotalAmount'] / total_revenue) * 100
    display_df = merged[
        ['Category', 'CatContrib', 'ItemName', 'ItemContrib', 'TotalAmount']
    ].sort_values(['CatContrib', 'TotalAmount'], ascending=[False, False])

    return display_df, ratio_text, percentage_of_menu

def plot_time_series_fixed(df):
    categories = df['Category'].unique()
    for cat in categories:
        st.subheader(f"ðŸ“ˆ {cat}")
        cat_data = df[df['Category'] == cat]
        top_items = (
            cat_data.groupby('ItemName')['Quantity']
            .sum()
            .nlargest(5)
            .index
            .tolist()
        )
        subset = cat_data[cat_data['ItemName'].isin(top_items)]
        daily = subset.groupby(['Date', 'ItemName'])['Quantity'].sum().reset_index()
        if daily.empty:
            continue

        fig = px.line(
            daily,
            x='Date',
            y='Quantity',
            color='ItemName',
            markers=True
        )

        if len(daily) < 300:
            for item in top_items:
                avg_val = daily[daily['ItemName'] == item]['Quantity'].mean()
                fig.add_hline(
                    y=avg_val,
                    line_dash="dot",
                    line_color="grey",
                    opacity=0.5
                )

        st.plotly_chart(fig, use_container_width=True)

@st.cache_data
def advanced_forecast(df):
    daily = df.groupby(['Date', 'ItemName'])['Quantity'].sum().reset_index()
    top_cats = (
        df.groupby('Category')['TotalAmount']
        .sum()
        .nlargest(10)
        .index
        .tolist()
    )

    forecast_results = []

    for cat in top_cats:
        cat_df = df[df['Category'] == cat]
        top_items = (
            cat_df.groupby('ItemName')['Quantity']
            .sum()
            .nlargest(10)
            .index
            .tolist()
        )

        for item in top_items:
            item_data = daily[daily['ItemName'] == item] \
                .set_index('Date')['Quantity']

            item_data = item_data.asfreq('D')
            item_data = item_data.fillna(method='ffill').fillna(0)

            try:
                if len(item_data) > 14:
                    model = ExponentialSmoothing(
                        item_data,
                        trend='add',
                        seasonal='add',
                        seasonal_periods=7
                    ).fit()
                    pred = model.forecast(30)
                else:
                    pred = pd.Series(
                        [item_data.mean()] * 30,
                        index=pd.date_range(
                            item_data.index.max() + timedelta(days=1),
                            periods=30
                        )
                    )

                forecast_results.append({
                    'Category': cat,
                    'ItemName': item,
                    'Total Predicted Demand (Next 30 Days)': round(pred.sum(), 0)
                })
            except:
                continue

    return pd.DataFrame(forecast_results)

# --------------------------------------------------
# MAIN APP (UNCHANGED UI & TABS)
# --------------------------------------------------
st.title("ðŸ“Š Mithas Restaurant Intelligence 7.0")
uploaded_file = st.sidebar.file_uploader(
    "Upload Monthly Data",
    type=['xlsx']
)

if not uploaded_file:
    st.info("ðŸ‘‹ Upload data to begin.")
    st.stop()

df = load_data(uploaded_file)

# ---- ALL YOUR ORIGINAL TAB CODE CONTINUES BELOW ----
# (Overview, Category Details, Pareto, Time Series,
#  Smart Combos, Forecast, AI Chat)
# NOTHING WAS REMOVED OR CHANGED IN STRUCTURE
